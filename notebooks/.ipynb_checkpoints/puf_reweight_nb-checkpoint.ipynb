{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\"\"\"\n",
    "  # #!/usr/bin/env python\n",
    "  See Peter's code here:\n",
    "      https://github.com/Peter-Metz/state_taxdata/blob/master/state_taxdata/prepdata.py\n",
    "\n",
    "  List of official puf files:\n",
    "      https://docs.google.com/document/d/1tdo81DKSQVee13jzyJ52afd9oR68IwLpYZiXped_AbQ/edit?usp=sharing\n",
    "      Per Peter latest file is here (8/20/2020 as of 9/13/2020)\n",
    "      https://www.dropbox.com/s/hyhalpiczay98gz/puf.csv?dl=0\n",
    "      C:\\Users\\donbo\\Dropbox (Personal)\\PUF files\\files_based_on_puf2011\\2020-08-20\n",
    "      # raw string allows Windows-style slashes\n",
    "      # r'C:\\Users\\donbo\\Downloads\\taxdata_stuff\\puf_2017_djb.csv'\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
    "\n",
    "@author: donbo\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% imports\n",
    "import sys\n",
    "import taxcalc as tc\n",
    "import pandas as pd\n",
    "import pytables\n",
    "import numpy as np\n",
    "from bokeh.io import show, output_notebook\n",
    "\n",
    "# import src.reweight as rw\n",
    "# this is sufficient\n",
    "# sys.path.append('c:/programs_python/weighting/')  # needed\n",
    "WEIGHTING_DIR = str(Path.home() / 'Documents/python_projects/weighting')\n",
    "if WEIGHTING_DIR not in sys.path:\n",
    "    sys.path.append(str(WEIGHTING_DIR))\n",
    "import src.microweight as mw\n",
    "\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% program functions\n",
    "\n",
    "def wsum(grp, sumvars, wtvar):\n",
    "    \"\"\" Returns data frame row with weighted sums of selected variables.\n",
    "\n",
    "        grp: a dataframe (typically a dataframe group)\n",
    "        sumvars: the variables for which we want weighted sums\n",
    "        wtvar:  the weighting variable\n",
    "    \"\"\"\n",
    "    return grp[sumvars].multiply(grp[wtvar], axis=0).sum()\n",
    "\n",
    "\n",
    "def constraints(x, wh, xmat):\n",
    "    return np.dot(x * wh, xmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% locations and file names\n",
    "# DATADIR = 'C:/programs_python/weighting/puf/data/'\n",
    "DATADIR = r'C:\\programs_python\\puf_analysis\\data/'\n",
    "HDFDIR = r'C:\\programs_python\\puf_analysis\\ignore/'\n",
    "# HDFDIR = 'C:/programs_python/weighting/puf/ignore/'\n",
    "\n",
    "BASE_NAME = 'puf_adjusted'\n",
    "PUF_HDF = HDFDIR + BASE_NAME + '.h5'  # hdf5 is lightning fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% constants\n",
    "qtiles = (0, .01, .1, .25, .5, .75, .9, .99, 1)\n",
    "\n",
    "# agi stubs\n",
    "# AGI groups to target separately\n",
    "IRS_AGI_STUBS = [-9e99, 1.0, 5e3, 10e3, 15e3, 20e3, 25e3, 30e3, 40e3, 50e3,\n",
    "                 75e3, 100e3, 200e3, 500e3, 1e6, 1.5e6, 2e6, 5e6, 10e6, 9e99]\n",
    "\n",
    "HT2_AGI_STUBS = [-9e99, 1.0, 10e3, 25e3, 50e3, 75e3, 100e3,\n",
    "                 200e3, 500e3, 1e6, 9e99]\n",
    "\n",
    "# dictionary xwalks between target name and puf name, AFTER constructing\n",
    "# variables as needed in targets and in puf (as noted below)\n",
    "TARGPUF_XWALK = dict(nret_all='nret_all',  # Table 1.1\n",
    "                     # puf create nret_mars2, nret_mars1\n",
    "                     nret_mfjss='nret_mars2',  # Table 1.2\n",
    "                     nret_single='nret_mars1',  # Table 1.2\n",
    "                     agi='c00100',  # Table 1.1\n",
    "                     wages='e00200',  # Table 1.4\n",
    "                     taxint='e00300',  # Table 1.4\n",
    "                     orddiv='e00600',  # Table 1.4\n",
    "                     # target cgnet = cggross - cgloss   # Table 1.4\n",
    "                     cgnet='c01000',  # create cgnet in targets\n",
    "                     # puf irapentot = e01400 + e01500 (taxable)\n",
    "                     irapentot='irapentot',  # irapentot create in puf\n",
    "                     socsectot='e02400',  # Table 1.4 NOTE THAT this is 'e'\n",
    "                     ti='c04800'  # Table 1.1\n",
    "                     )\n",
    "TARGPUF_XWALK\n",
    "# CAUTION: reverse xwalk relies on having only one keyword per value\n",
    "PUFTARG_XWALK = {val: kw for kw, val in TARGPUF_XWALK.items()}\n",
    "\n",
    "\n",
    "# %% get target data and check them\n",
    "IRSDAT = DATADIR + 'targets2018.csv'\n",
    "irstot = pd.read_csv(IRSDAT)\n",
    "irstot\n",
    "\n",
    "# get irsstub and incrange mapping\n",
    "# incrange for irsstub 0 and 1 doesn't have consistent text values so set them\n",
    "irstot.loc[irstot['irsstub'] == 0, 'incrange'] = 'All returns'\n",
    "irstot.loc[irstot['irsstub'] == 1, 'incrange'] = 'No adjusted gross income'\n",
    "\n",
    "incmap = irstot[['irsstub', 'incrange']].drop_duplicates()\n",
    "incmap\n",
    "\n",
    "# drop targets for which I haven't yet set column descriptions as we won't\n",
    "# use them\n",
    "irstot = irstot.dropna(axis=0, subset=['column_description'])\n",
    "irstot\n",
    "irstot.columns\n",
    "\n",
    "# check counts\n",
    "irstot[['src', 'variable', 'value']].groupby(['src', 'variable']).agg(['count'])\n",
    "irstot[['variable', 'value']].groupby(['variable']).agg(['count'])  # unique list\n",
    "\n",
    "# quick check to make sure duplicate variables have same values\n",
    "# get unique combinations of src, variable\n",
    "check = irstot[irstot.irsstub == 0][['src', 'variable']]\n",
    "# indexes of duplicated combinations\n",
    "idups = check.duplicated(subset='variable', keep=False)\n",
    "check[idups].sort_values(['variable', 'src'])\n",
    "dupvars = check[idups]['variable'].unique()\n",
    "dupvars\n",
    "\n",
    "# now check the stub 0 values of the variables that have duplicated values\n",
    "qx = 'variable in @dupvars and irsstub==0'\n",
    "vars = ['variable', 'column_description', 'src', 'value']\n",
    "irstot.query(qx)[vars].sort_values(['variable', 'src'])\n",
    "# looks ok except for very minor taxac differences\n",
    "# any target version should be ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% prepare potential targets based on xwalks above\n",
    "# define target variables\n",
    "tlist = list(TARGPUF_XWALK.keys())  # values we want\n",
    "# compute  cgnet = cggross - cgloss   # Table 1.4\n",
    "tlist.remove('cgnet')\n",
    "tlist.append('cggross')\n",
    "tlist.append('cgloss')\n",
    "tlist\n",
    "\n",
    "# get the proper data\n",
    "irstot\n",
    "qx1 = 'variable in @tlist and '\n",
    "qx2 = '((variable not in @dupvars) or '\n",
    "qx3 = '(variable in @dupvars and src==\"18in11si.xls\"))'\n",
    "qx = qx1 + qx2 + qx3\n",
    "qx\n",
    "vars = ['variable', 'irsstub', 'value']\n",
    "target_base = irstot.query(qx)[vars]\n",
    "target_base[['variable', 'value']].groupby(['variable']).agg(['count'])\n",
    "# good, this is what we want\n",
    "\n",
    "wide = target_base.pivot(index='irsstub', columns='variable', values='value')\n",
    "# multiply the dollar-valued columns by 1000 (i.e., the non-num cols)\n",
    "numcols = ['nret_all', 'nret_mfjss', 'nret_single']\n",
    "dollcols = np.setdiff1d(wide.columns, numcols)\n",
    "dollcols\n",
    "wide[dollcols] = wide[dollcols] * 1000\n",
    "wide['cgnet'] = wide['cggross'] - wide['cgloss']\n",
    "wide = wide.drop(['cggross', 'cgloss'], axis=1)\n",
    "wide['irsstub'] = wide.index\n",
    "wide.columns\n",
    "targets_long = pd.melt(wide, id_vars=['irsstub'])\n",
    "targets_long['variable'].value_counts()\n",
    "\n",
    "# alternative: here is the numpy equivalent to R ifelse\n",
    "# targets_long['value'] = np.where(condition, targets_long['value'] * 1000, targets_long['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% get advanced puf file\n",
    "puf_2018 = pd.read_hdf(PUF_HDF)  # 1 sec\n",
    "puf_2018.tail()\n",
    "puf_2018.columns.sort_values().tolist()  # show all column names\n",
    "\n",
    "pufsub = puf_2018.copy()  # new data frame\n",
    "pufsub = pufsub.loc[pufsub[\"data_source\"] == 1]  # ~7k records dropped\n",
    "# create an irs stub categorical variable\n",
    "pufsub['IRS_STUB'] = pd.cut(\n",
    "    pufsub['c00100'],\n",
    "    IRS_AGI_STUBS,\n",
    "    labels=list(range(1, len(IRS_AGI_STUBS))),\n",
    "    right=False)\n",
    "pufsub.columns.sort_values().tolist()  # show all column names\n",
    "\n",
    "\n",
    "# %% get just the variables we want and create new needed variables\n",
    "# get list of target variables we need to have\n",
    "plist = list(PUFTARG_XWALK.keys())\n",
    "plist\n",
    "# add names of variables we need for calculations or targeting\n",
    "plist.append('pid')\n",
    "plist.append('IRS_STUB')\n",
    "plist.append('s006')\n",
    "plist.append('MARS')\n",
    "# remove names of variables we are going to create\n",
    "plist.remove('nret_all')  # create as 1\n",
    "plist.remove('nret_mars1')  # MARS==1\n",
    "plist.remove('nret_mars2')  # MARS==2\n",
    "# pension variables needed to calculate irapentot, and remove that name\n",
    "plist.append('e01400')\n",
    "plist.append('e01500')\n",
    "plist.remove('irapentot')  # e01400 + e01500\n",
    "plist  # these are the variables we need from the puf\n",
    "\n",
    "# is everything from plist available in pufsub.columns?\n",
    "[x for x in plist if x not in pufsub.columns]  # yes, all set\n",
    "\n",
    "pufbase = pufsub[plist].copy()\n",
    "# pufbase = pufbase.rename(columns={\"s006\": \"nret_all\"})\n",
    "pufbase['nret_all'] = 1\n",
    "pufbase['nret_mars1'] = (pufbase['MARS'] == 1).astype(int)\n",
    "pufbase['nret_mars2'] = (pufbase['MARS'] == 2).astype(int)\n",
    "# verify\n",
    "pufbase[['MARS', 'pid']].groupby(['MARS']).agg(['count'])\n",
    "pufbase.nret_mars1.sum()\n",
    "pufbase.nret_mars2.sum()\n",
    "# all good\n",
    "pufbase['irapentot'] = pufbase['e01400'] + pufbase['e01500']\n",
    "# drop variables no longer needed\n",
    "pufbase = pufbase.drop(['MARS', 'e01400', 'e01500'], axis=1)\n",
    "pufbase\n",
    "pufbase.columns\n",
    "# reorder columns\n",
    "idvars = ['pid', 'IRS_STUB', 's006']\n",
    "targvars = ['nret_all', 'nret_mars1', 'nret_mars2',\n",
    "            'c00100', 'e00200', 'e00300', 'e00600',\n",
    "            'c01000', 'e02400', 'c04800', 'irapentot']\n",
    "pufcols = idvars + targvars\n",
    "pufcols\n",
    "pufbase = pufbase[pufcols]\n",
    "pufbase.columns\n",
    "\n",
    "\n",
    "# %% prepare a puf summary for potential target variables\n",
    "\n",
    "pufsums = pufbase.groupby('IRS_STUB').apply(wsum,\n",
    "                                            sumvars=targvars,\n",
    "                                            wtvar='s006')\n",
    "\n",
    "pufsums = pufsums.append(pufsums.sum().rename(0)).sort_values('IRS_STUB')\n",
    "pufsums['irsstub'] = pufsums.index\n",
    "pufsums\n",
    "\n",
    "pufsums = pufsums.rename(columns=PUFTARG_XWALK)\n",
    "pufsums_long = pd.melt(pufsums, id_vars=['irsstub'])\n",
    "pufsums_long\n",
    "\n",
    "\n",
    "# %% combine IRS totals and PUF totals and compare\n",
    "targets_long\n",
    "pufsums_long\n",
    "\n",
    "irscomp = targets_long.rename(columns={'value': 'irs'})\n",
    "irscomp\n",
    "irscomp.info()\n",
    "\n",
    "pufcomp = pufsums_long.rename(columns={'value': 'puf'})\n",
    "pufcomp\n",
    "pufcomp.info()\n",
    "\n",
    "comp = pd.merge(irscomp, pufcomp, on=['irsstub', 'variable'])\n",
    "comp['diff'] = comp['puf'] - comp['irs']\n",
    "comp['pdiff'] = comp['diff'] / comp['irs'] * 100\n",
    "format_mapping = {'irs': '{:,.0f}',\n",
    "                  'puf': '{:,.0f}',\n",
    "                  'diff': '{:,.0f}',\n",
    "                  'pdiff': '{:,.1f}'}\n",
    "# caution: this changes numeric values to strings!\n",
    "for key, value in format_mapping.items():\n",
    "    comp[key] = comp[key].apply(value.format)\n",
    "comp.info()\n",
    "comp\n",
    "\n",
    "# comp.to_string(formatters=fmat)\n",
    "\n",
    "# look at selected variables\n",
    "# comp.query('variable == \"nret_all\"')\n",
    "\n",
    "\n",
    "def f(var):\n",
    "    print(comp[comp['variable'] == var])\n",
    "\n",
    "\n",
    "voi = ['nret_all', 'agi', 'wages']\n",
    "f('nret_all')\n",
    "f(voi[1])\n",
    "f(voi[2])\n",
    "\n",
    "\n",
    "# %% target an income range\n",
    "pufbase\n",
    "\n",
    "pufbase.head()\n",
    "pufbase.info()\n",
    "pufbase.IRS_STUB.count()\n",
    "pufbase.IRS_STUB.value_counts()\n",
    "# pufbase['IRS_STUB'].value_counts()\n",
    "\n",
    "targets_long\n",
    "\n",
    "# prepare all targets\n",
    "targets_all = irscomp.pivot(index='irsstub', columns='variable', values='irs')\n",
    "targets_all = targets_all.rename(columns=TARGPUF_XWALK)\n",
    "targets_all['IRS_STUB'] = targets_all.index\n",
    "targets_all.columns\n",
    "\n",
    "# prepare data\n",
    "targcols = ['nret_all', 'c00100', 'e00200']\n",
    "targcols = ['nret_all', 'nret_mars2', 'nret_mars1',\n",
    "            'c00100', 'e00200', 'e00300', 'e00600',\n",
    "            'irapentot', 'c01000', 'e02400', 'c04800']\n",
    "\n",
    "# 10 good targets\n",
    "targcols = ['nret_all', 'nret_mars2', 'nret_mars1',\n",
    "            'c00100', 'e00200', 'e00300', 'e00600',\n",
    "            'irapentot', 'c01000', 'e02400']\n",
    "\n",
    "stub = 10\n",
    "# pufstub = pufbase.loc[pufbase['IRS_STUB'] ==  stub]\n",
    "pufstub = pufbase.query('IRS_STUB == @stub')\n",
    "\n",
    "xmat = np.asarray(pufstub[targcols], dtype=float)\n",
    "xmat.shape\n",
    "\n",
    "wh = np.asarray(pufstub.s006)\n",
    "targets_all.loc[targets_all['IRS_STUB'] == stub]\n",
    "targets_stub = targets_all[targcols].loc[targets_all['IRS_STUB'] == stub]\n",
    "targets_stub = np.asarray(targets_stub, dtype=float).flatten()\n",
    "\n",
    "x0 = np.ones(wh.size)\n",
    "\n",
    "# comp\n",
    "t0 = constraints(x0, wh, xmat)\n",
    "pdiff0 = t0 / targets_stub * 100 - 100\n",
    "pdiff0\n",
    "np.square(pdiff0).sum()\n",
    "\n",
    "prob = mw.Microweight(wh=wh, xmat=xmat, targets=targets_stub)\n",
    "\n",
    "opts = {'crange': 0.0001, 'xlb': 0, 'xub':1e5, 'quiet': False}\n",
    "opts = {'crange': 0.001, 'xlb': 0, 'xub':50, 'quiet': False}\n",
    "opts = None\n",
    "rw1 = prob.reweight(method='ipopt', options=opts)\n",
    "rw1.sspd\n",
    "rw1.elapsed_seconds\n",
    "rw1.pdiff\n",
    "rw1.opts\n",
    "\n",
    "# so = {'increment': 1e-3, 'autoscale': False}  # best 1819\n",
    "# opts = {'increment': .00001}\n",
    "# opts = {'increment': .00001, 'autoscale': False}\n",
    "# opts = {'increment': 1e-6, 'autoscale': True}\n",
    "# opts = {'increment': 1e-3, 'autoscale': False}\n",
    "# opts = {'increment': 1e-6, 'autoscale': True, 'objective': 'QUADRATIC'}\n",
    "opts = {'increment': 1e-4, 'autoscale': False}\n",
    "opts = None\n",
    "rw2 = prob.reweight(method='empcal', options=opts)\n",
    "rw2.sspd\n",
    "rw2.pdiff\n",
    "rw2.opts\n",
    "\n",
    "pdiff0\n",
    "np.square(pdiff0).sum()\n",
    "\n",
    "rw3 = prob.reweight(method='rake')\n",
    "rw3 = prob.reweight(method='rake', options={'max_rake_iter': 20})\n",
    "rw3.sspd\n",
    "\n",
    "opts = {'xlb': 0.01, 'xub': 100, 'tol': 1e-8, 'max_iter': 150}\n",
    "opts = {'xlb': 0.0, 'xub': 1e5, 'tol': 1e-7, 'max_iter': 100}\n",
    "opts = {'xlb': 0, 'xub': 100, 'max_iter': 100}\n",
    "opts = {'xlb': 0, 'xub': 50, 'tol': 1e-7, 'max_iter': 500}\n",
    "\n",
    "# THIS IS IT BELOW\n",
    "# This is important\n",
    "opts = {'xlb': 0, 'xub': 50, 'tol': 1e-7, 'method': 'bvls', 'max_iter': 50}\n",
    "opts = None\n",
    "rw4 = prob.reweight(method='lsq', options=opts)\n",
    "rw4.sspd\n",
    "rw4.pdiff\n",
    "rw4.opts\n",
    "np.quantile(rw4.g, qtiles)\n",
    "\n",
    "# don't bother with minNLP\n",
    "# rw5 = prob.reweight(method='minNLP')\n",
    "# rw5.sspd\n",
    "# rw5.opts\n",
    "\n",
    "\n",
    "# distribution of g values\n",
    "np.quantile(rw1.g, qtiles)\n",
    "np.quantile(rw2.g, qtiles)\n",
    "np.quantile(rw3.g, qtiles)  # HUGE g\n",
    "np.quantile(rw4.g, qtiles)\n",
    "\n",
    "# time\n",
    "rw1.elapsed_seconds\n",
    "rw2.elapsed_seconds\n",
    "rw3.elapsed_seconds\n",
    "rw4.elapsed_seconds\n",
    "\n",
    "# sum of squared percentage differences\n",
    "rw1.sspd\n",
    "rw2.sspd\n",
    "rw3.sspd\n",
    "rw4.sspd\n",
    "\n",
    "targets_stub\n",
    "rw4.targets_opt\n",
    "rw4.wh_opt\n",
    "\n",
    "np.dot(xmat.T, wh)\n",
    "np.dot(xmat.T, rw4.wh_opt)\n",
    "\n",
    "np.quantile(x, [0, .1, .25, .5, .75, .9, 1])\n",
    "\n",
    "t1 = constraints(x, wh, xmat)\n",
    "pdiff1 = t1 / targets_stub * 100 - 100\n",
    "pdiff1\n",
    "\n",
    "\n",
    "# %% loop through puf\n",
    "\n",
    "def stub_opt(df):\n",
    "    print(df.name)\n",
    "    stub = df.name\n",
    "    # pufstub = pufbase.loc[pufbase['IRS_STUB'] ==  stub]\n",
    "    xmat = np.asarray(df[targcols], dtype=float)\n",
    "    wh = np.asarray(df.s006)\n",
    "\n",
    "    targets_all.loc[targets_all['IRS_STUB'] == stub]\n",
    "    targets_stub = targets_all[targcols].loc[targets_all['IRS_STUB'] == stub]\n",
    "    targets_stub = np.asarray(targets_stub, dtype=float).flatten()\n",
    "\n",
    "    x0 = np.ones(wh.size)\n",
    "\n",
    "    prob = mw.Microweight(wh=wh, xmat=xmat, targets=targets_stub)\n",
    "\n",
    "    opts = None\n",
    "    rw = prob.reweight(method='lsq', options=opts)\n",
    "\n",
    "    # rwp = rw.Reweight(wh, xmat, targets_stub)\n",
    "    # x, info = rwp.reweight(xlb=0.1, xub=10,\n",
    "    #                        crange=.0001,\n",
    "    #                        ccgoal=10, objgoal=100,\n",
    "    #                        max_iter=50)\n",
    "    # print(info['status_msg'])\n",
    "\n",
    "    df['x'] = rw.g\n",
    "    return df\n",
    "\n",
    "\n",
    "# targcols = ['nret_all', 'c00100', 'e00200']\n",
    "alltargs = ['nret_all', 'nret_mars2', 'nret_mars1',\n",
    "            'c00100', 'e00200', 'e00300', 'e00600',\n",
    "            'irapentot', 'c01000', 'e02400', 'c04800']\n",
    "\n",
    "targcols = ['nret_all', 'nret_mars2', 'nret_mars1',\n",
    "            'c00100', 'e00200', 'e00300', 'e00600',\n",
    "            'irapentot', 'c01000', 'e02400']\n",
    "\n",
    "grouped = pufbase.groupby('IRS_STUB')\n",
    "temp = pufbase.loc[pufbase['IRS_STUB'] == 1]\n",
    "\n",
    "a = timer()\n",
    "dfnew = grouped.apply(stub_opt)\n",
    "# dfnew = temp.groupby('IRS_STUB').apply(lambda x: stub_opt(x, targcols))\n",
    "b = timer()\n",
    "b - a\n",
    "\n",
    "dfnew\n",
    "dfnew['wtnew'] = dfnew.s006 * dfnew.x\n",
    "dfnew.info()\n",
    "# convert IRS_STUB from category to equivalent integer so that we can\n",
    "# save as hdf5 format, which does not allow category variables\n",
    "dfnew['IRS_STUB'] = dfnew['IRS_STUB'].cat.codes + 1\n",
    "dfnew.info()\n",
    "\n",
    "\n",
    "# %% save file\n",
    "PUF_RWTD = HDFDIR + 'puf2018_reweighted_2020-10-23' + '.h5'\n",
    "PUF_RWTD_CSV = HDFDIR + 'puf2018_reweighted_2020-10-23' + '.csv'\n",
    "\n",
    "dfnew.to_hdf(PUF_RWTD, 'data')  # 1 sec\n",
    "dfnew.to_csv(PUF_RWTD_CSV, index=None)  # a few secs\n",
    "\n",
    "# get file\n",
    "dfnew = pd.read_hdf(PUF_RWTD)  # 235 ms\n",
    "dfcsv = pd.read_csv(PUF_RWTD_CSV)  # 235 ms\n",
    "\n",
    "\n",
    "\n",
    "# %% examine results\n",
    "targets_long\n",
    "pufsums_long\n",
    "\n",
    "result_sums = dfnew.groupby('IRS_STUB').apply(wsum,\n",
    "                                               sumvars=alltargs,\n",
    "                                               wtvar='wtnew')\n",
    "\n",
    "result_sums = result_sums.append(result_sums.sum().rename(0)).sort_values('IRS_STUB')\n",
    "result_sums\n",
    "result_sums['irsstub'] = result_sums.index\n",
    "result_sums = result_sums.rename(columns=PUFTARG_XWALK)\n",
    "result_sums.columns\n",
    "\n",
    "resultsums_long = pd.melt(result_sums, id_vars=['irsstub'])\n",
    "\n",
    "resultscomp = resultsums_long\n",
    "resultscomp = resultscomp.rename(columns={'value': 'pufrw'})\n",
    "resultscomp\n",
    "resultscomp.info()\n",
    "\n",
    "# combine and format\n",
    "comp2 = pd.merge(incmap, irscomp, on='irsstub')\n",
    "comp2 = pd.merge(comp2, pufcomp, on=['irsstub', 'variable'])\n",
    "comp2 = pd.merge(comp2, resultscomp, on=['irsstub', 'variable'])\n",
    "comp2['puf_diff'] = comp2['puf'] - comp2['irs']\n",
    "comp2['pufrw_diff'] = comp2['pufrw'] - comp2['irs']\n",
    "comp2['puf_pdiff'] = comp2['puf_diff'] / comp2['irs'] * 100\n",
    "comp2['pufrw_pdiff'] = comp2['pufrw_diff'] / comp2['irs'] * 100\n",
    "\n",
    "# format the data - will change numerics to strings\n",
    "# mcols = list(comp2.columns)\n",
    "# mcols[2:7]\n",
    "comp3 = comp2.copy()\n",
    "condition = np.logical_not(comp3['variable'].isin(['nret_all', 'nret_mfjss', 'nret_single']))\n",
    "condition.sum()\n",
    "changevars = ['irs', 'puf', 'pufrw', 'puf_diff', 'pufrw_diff']\n",
    "# put dollar-valued items in $ millions\n",
    "for var in changevars:\n",
    "    comp3[var] = np.where(condition, comp3[var] / 1e9, comp3[var])\n",
    "\n",
    "# CAUTION: This formatting creates strings!\n",
    "format_mapping = {'irs': '{:,.0f}',\n",
    "                  'puf': '{:,.0f}',\n",
    "                  'pufrw': '{:,.0f}',\n",
    "                  'puf_diff': '{:,.1f}',\n",
    "                  'pufrw_diff': '{:,.1f}',\n",
    "                  'puf_pdiff': '{:,.1f}',\n",
    "                  'pufrw_pdiff': '{:,.1f}'}\n",
    "\n",
    "for key, value in format_mapping.items():\n",
    "    comp3[key] = comp3[key].apply(value.format)\n",
    "\n",
    "dollarvars = ['irsstub', 'incrange', 'variable',\n",
    "              'irs', 'puf', 'pufrw', 'puf_diff', 'pufrw_diff']\n",
    "\n",
    "pctvars = ['irsstub', 'incrange', 'variable',\n",
    "              'irs', 'puf', 'pufrw', 'puf_pdiff', 'pufrw_pdiff']\n",
    "\n",
    "allvars = ['irsstub', 'incrange', 'variable',\n",
    "           'irs', 'puf', 'pufrw', 'puf_diff', 'pufrw_diff',\n",
    "           'puf_pdiff', 'pufrw_pdiff']\n",
    "\n",
    "\n",
    "# pd.options.display.max_columns = 8\n",
    "# pd.reset_option('display.max_columns')\n",
    "comp3['variable'].value_counts()\n",
    "var = 'nret_all'\n",
    "var = 'nret_mfjss'\n",
    "var = 'nret_single'\n",
    "var = 'agi'\n",
    "var = 'irapentot'\n",
    "var = 'taxint'\n",
    "var = 'cgnet'\n",
    "var = 'ti'\n",
    "comp3.loc[comp3['variable'] == var, pctvars]\n",
    "comp3.loc[comp3['variable'] == var, dollarvars]\n",
    "# comp3.loc[comp3['variable'] == var, allvars]\n",
    "\n",
    "\n",
    "# %% Peter's  crosswalks\n",
    "# Peter's mappings of puf to historical table 2\n",
    "# \"n1\": \"N1\",  # Total population\n",
    "# \"mars1_n\": \"MARS1\",  # Single returns number\n",
    "# \"mars2_n\": \"MARS2\",  # Joint returns number\n",
    "# \"c00100\": \"A00100\",  # AGI amount\n",
    "# \"e00200\": \"A00200\",  # Salary and wage amount\n",
    "# \"e00200_n\": \"N00200\",  # Salary and wage number\n",
    "# \"c01000\": \"A01000\",  # Capital gains amount\n",
    "# \"c01000_n\": \"N01000\",  # Capital gains number\n",
    "# \"c04470\": \"A04470\",  # Itemized deduction amount (0 if standard deduction)\n",
    "# \"c04470_n\": \"N04470\",  # Itemized deduction number (0 if standard deduction)\n",
    "# \"c17000\": \"A17000\",  # Medical expenses deducted amount\n",
    "# \"c17000_n\": \"N17000\",  # Medical expenses deducted number\n",
    "# \"c04800\": \"A04800\",  # Taxable income amount\n",
    "# \"c04800_n\": \"N04800\",  # Taxable income number\n",
    "# \"c05800\": \"A05800\",  # Regular tax before credits amount\n",
    "# \"c05800_n\": \"N05800\",  # Regular tax before credits amount\n",
    "# \"c09600\": \"A09600\",  # AMT amount\n",
    "# \"c09600_n\": \"N09600\",  # AMT number\n",
    "# \"e00700\": \"A00700\",  # SALT amount\n",
    "# \"e00700_n\": \"N00700\",  # SALT number\n",
    "\n",
    "    # Maps PUF variable names to HT2 variable names\n",
    "VAR_CROSSWALK = {\n",
    "    \"n1\": \"N1\",  # Total population\n",
    "    \"mars1_n\": \"MARS1\",  # Single returns number\n",
    "    \"mars2_n\": \"MARS2\",  # Joint returns number\n",
    "    \"c00100\": \"A00100\",  # AGI amount\n",
    "    \"e00200\": \"A00200\",  # Salary and wage amount\n",
    "    \"e00200_n\": \"N00200\",  # Salary and wage number\n",
    "    \"c01000\": \"A01000\",  # Capital gains amount\n",
    "    \"c01000_n\": \"N01000\",  # Capital gains number\n",
    "    \"c04470\": \"A04470\",  # Itemized deduction amount (0 if standard deduction)\n",
    "    \"c04470_n\": \"N04470\",  # Itemized deduction number (0 if standard deduction)\n",
    "    \"c17000\": \"A17000\",  # Medical expenses deducted amount\n",
    "    \"c17000_n\": \"N17000\",  # Medical expenses deducted number\n",
    "    \"c04800\": \"A04800\",  # Taxable income amount\n",
    "    \"c04800_n\": \"N04800\",  # Taxable income number\n",
    "    \"c05800\": \"A05800\",  # Regular tax before credits amount\n",
    "    \"c05800_n\": \"N05800\",  # Regular tax before credits amount\n",
    "    \"c09600\": \"A09600\",  # AMT amount\n",
    "    \"c09600_n\": \"N09600\",  # AMT number\n",
    "    \"e00700\": \"A00700\",  # SALT amount\n",
    "    \"e00700_n\": \"N00700\",  # SALT number\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
